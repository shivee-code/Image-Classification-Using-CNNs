# -*- coding: utf-8 -*-
"""Image Classification Using CNNs.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zUiRZj-dUrzLaSPipLZaC0p2XaBwja-I

# **Data Exploration and Preparation**
"""

# Import library

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import cifar10
import seaborn as sns

"""**Data Exploration and Preparation**"""

# Load the dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

# Show shapes
print("Training data shape:", x_train.shape)
print("Test data shape:", x_test.shape)

# CIFAR-10 class names
class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Display 5 sample images with labels
plt.figure(figsize=(10,2))
for i in range(5):
    plt.subplot(1, 5, i+1)
    plt.imshow(x_train[i])
    plt.title(class_names[int(y_train[i])])
    plt.axis('off')
plt.show()

"""**Normalize Images and Split Data**"""

# Normalize pixel values to [0, 1]
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten labels
y_train = y_train.flatten()
y_test = y_test.flatten()

# Check label distribution
print("Unique labels:", np.unique(y_train))
print("Label counts:", np.bincount(y_train))

"""# **Build and Train a CNN Model**

**Build CNN Model**
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.utils import to_categorical

# One-hot encode the labels
y_train_cat = to_categorical(y_train, 10)
y_test_cat = to_categorical(y_test, 10)

# Build the CNN model
model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Show model summary
model.summary()

"""**Compile and Train Model**"""

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train_cat, epochs=15, batch_size=64,
                    validation_split=0.2, verbose=1)

"""**Plot Accuracy and Loss**"""

plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.legend()
plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Val Loss')
plt.legend()
plt.title("Loss")
plt.show()

##  Comment: If the validation accuracy is much lower than training, overfitting is occurring. If both are low, the model might be underfitting.

"""# **Evaluate the Model**

**Evaluation on Test Set**
"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

# Evaluate
test_loss, test_accuracy = model.evaluate(x_test, y_test_cat)
print("Test Accuracy:", test_accuracy)

# Predict labels
y_pred = model.predict(x_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Classification report
print(classification_report(y_test, y_pred_classes, target_names=class_names))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred_classes)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

"""**Show the Examples**"""

# Show 5 correct and 5 incorrect predictions
correct = np.where(y_pred_classes == y_test)[0]
incorrect = np.where(y_pred_classes != y_test)[0]

# Show correct examples
plt.figure(figsize=(10,2))
for i, idx in enumerate(correct[:5]):
    plt.subplot(1,5,i+1)
    plt.imshow(x_test[idx])
    plt.title(f"True: {class_names[y_test[idx]]}\nPred: {class_names[y_pred_classes[idx]]}")
    plt.axis('off')
plt.suptitle("Correct Predictions")
plt.show()

# Show incorrect examples
plt.figure(figsize=(10,2))
for i, idx in enumerate(incorrect[:5]):
    plt.subplot(1,5,i+1)
    plt.imshow(x_test[idx])
    plt.title(f"True: {class_names[y_test[idx]]}\nPred: {class_names[y_pred_classes[idx]]}")
    plt.axis('off')
plt.suptitle("Incorrect Predictions")
plt.show()

"""# **Model Improvement**

**Try a Different Optimizer**
"""

# Build same model again
model_sgd = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D((2, 2)),
    Dropout(0.25),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Compile with SGD
from tensorflow.keras.optimizers import SGD
model_sgd.compile(optimizer=SGD(), loss='categorical_crossentropy', metrics=['accuracy'])

# Train
history_sgd = model_sgd.fit(x_train, y_train_cat, epochs=15, batch_size=64,
                            validation_split=0.2, verbose=1)

"""**Compare Performance**"""

# Evaluate SGD model
test_loss_sgd, test_acc_sgd = model_sgd.evaluate(x_test, y_test_cat)

# Comparison Table
print("Optimizer Comparison")
print(f"Adam Test Accuracy: {test_accuracy:.4f}")
print(f"SGD Test Accuracy: {test_acc_sgd:.4f}")

